{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoSRXsn_HwYs",
        "outputId": "92356cd4-cfd6-4271-93ee-0afaf859bb4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  target  \\\n",
            "0  From: juvirtan@klaava.Helsinki.FI (Jukka A Vir...       2   \n",
            "1  From: kozloce@wkuvx1.bitnet\\nSubject: Re: Tie ...       2   \n",
            "2  Organization: University of Notre Dame - Offic...       1   \n",
            "3  From: henry@zoo.toronto.edu (Henry Spencer)\\nS...       3   \n",
            "4  From: m_klein@pavo.concordia.ca (CorelMARK!)\\n...       1   \n",
            "\n",
            "          target_name  \n",
            "0    rec.sport.hockey  \n",
            "1    rec.sport.hockey  \n",
            "2  rec.sport.baseball  \n",
            "3           sci.space  \n",
            "4  rec.sport.baseball  \n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import pandas as pd\n",
        "\n",
        "categories = [\n",
        "    'comp.graphics',\n",
        "    'rec.sport.baseball',\n",
        "    'sci.space',\n",
        "    'talk.politics.mideast',\n",
        "    'rec.sport.hockey'   # added new category\n",
        "]\n",
        "\n",
        "newsgroups = fetch_20newsgroups(subset='train', categories=categories)\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'text': newsgroups.data,\n",
        "    'target': newsgroups.target\n",
        "})\n",
        "\n",
        "df['target_name'] = df['target'].apply(lambda x: newsgroups.target_names[x])\n",
        "\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Article Distribution by each category"
      ],
      "metadata": {
        "id": "5fVFD8i_KLI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  print(\"Category \",i,\" : \",len(df[df['target']==i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYjD1ZJzI_vy",
        "outputId": "84762fa8-f7be-422e-e51b-8c5d2063b874"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Category  0  :  584\n",
            "Category  1  :  597\n",
            "Category  2  :  600\n",
            "Category  3  :  593\n",
            "Category  4  :  564\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample articles from each category  "
      ],
      "metadata": {
        "id": "O-GtVRN4KRzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  print(f\"Category {i} {df[df['target']==i]['target_name'].iloc[0]}: {df[df['target']==i]['text'].iloc[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tfM1ObpJp_-",
        "outputId": "f6136441-bf3c-4057-b58c-013ce00da6e9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Category 0 comp.graphics: From: ajackson@cch.coventry.ac.uk (Alan Jackson)\n",
            "Subject: MPEG Location\n",
            "Nntp-Posting-Host: cc_sysh\n",
            "Organization: Coventry University\n",
            "Lines: 11\n",
            "\n",
            "\n",
            "Can anyone tell me where to find a MPEG viewer (either DOS or\n",
            "Windows).\n",
            "\n",
            "Thanks in advance.\n",
            "\n",
            "-- \n",
            "Alan M. Jackson      Mail : ajackson@cch.cov.ac.uk\n",
            "\n",
            "     Liverpool Football Club - Simply The Best\n",
            "              \"You'll Never Walk Alone\"\n",
            "\n",
            "Category 1 rec.sport.baseball: Organization: University of Notre Dame - Office of Univ. Computing\n",
            "From: <RVESTERM@vma.cc.nd.edu>\n",
            "Subject: Re: NL vs. AL?\n",
            " <93102.164224RVESTERM@vma.cc.nd.edu> <1993Apr13.184311.16351@news.yale.edu>\n",
            "Lines: 23\n",
            "\n",
            "In article <1993Apr13.184311.16351@news.yale.edu>, (Sean Garrison) says:\n",
            ">\n",
            ">In article <93102.164224RVESTERM@vma.cc.nd.edu>, RVESTERM@vma.cc.nd.edu\n",
            ">wrote:\n",
            ">\n",
            ">> pitchers who are doing well are\n",
            ">> more likely to be taken out of the game in the nl than they are in the al,\n",
            ">> so it seems to me that the al, not the nl, promotes pitchers' duels.\n",
            ">>\n",
            ">> bob vesterman.\n",
            ">\n",
            ">\n",
            ">On what basis do you make this statement?\n",
            ">\n",
            ">                                Q Sean\n",
            "\n",
            "are you serious? pitchers are pinch-hit for in the nl.  they are not in the\n",
            "nl.  if a pitcher is cranking in the al, he will stay in the game.  if he\n",
            "is cranking in the nl, he may not - ESPECIALLY if it's a pitchers' duel,\n",
            "and his team needs an extra run.\n",
            "\n",
            "bob vesterman.\n",
            "\n",
            "\n",
            "Category 2 rec.sport.hockey: From: juvirtan@klaava.Helsinki.FI (Jukka A Virtanen)\n",
            "Subject: Re: Plus minus stat\n",
            "Organization: University of Helsinki\n",
            "Lines: 24\n",
            "\n",
            "In <1993Apr16.015936.11303@ramsey.cs.laurentian.ca> maynard@ramsey.cs.laurentian.ca (Roger Maynard) writes:\n",
            "\n",
            ">>>Good for you.  You'd only be displaying your ignorance of\n",
            ">>>course, but to each his own...\n",
            ">> \n",
            ">>Roger, I'm not sure here, but I think \"ignorance\" is really a\n",
            ">>function of \"a lack of knowledge\" and not \"formulating an\n",
            ">>opinion\"...but hey, if you need to take a cheap shot, then by all\n",
            ">>means go ahead...that's if it makes you feel better.\n",
            "\n",
            ">To knowledgeable observers of the game my meaning is obvious.  Your\n",
            ">hockey education is not my responsibility.\n",
            " \n",
            "Just curious, Roger, but since you have such a vast knowledge of the\n",
            "game and the league, how come you haven't made a living out of it?\n",
            "There must be a lot of demand for expertise in the field. I'm sure\n",
            "you'd be of great help to, say, the Leafs as an assistant coach or\n",
            "a scout. Or maybe try a career as a reporter or tv commentator...\n",
            " \n",
            "I might be wrong, of course, and you already have.\n",
            "-- \n",
            "Jukka A Virtanen\n",
            "                                                    juvirtan@cc.helsinki.fi\n",
            "                       University of Helsinki\n",
            "\n",
            "Category 3 sci.space: From: henry@zoo.toronto.edu (Henry Spencer)\n",
            "Subject: Re: Moonbase race\n",
            "Organization: U of Toronto Zoology\n",
            "Lines: 13\n",
            "\n",
            "In article <1r6rn3INNn96@mojo.eng.umd.edu> sysmgr@king.eng.umd.edu writes:\n",
            ">You'd need to launch HLVs to send up large amounts of stuff.  Do you know \n",
            ">of a private Titan pad? \n",
            "\n",
            "You'd need to launch HLVs to send up large amounts of stuff *if* you assume\n",
            "no new launcher development.  If you assume new launcher development, with\n",
            "lower costs as a specific objective, then you probably don't want to\n",
            "build something HLV-sized anyway.\n",
            "\n",
            "Nobody who is interested in launching things cheaply will buy Titans.  It\n",
            "doesn't take many Titan pricetags to pay for a laser launcher or a large\n",
            "gas gun or a development program for a Big Dumb Booster, all of which\n",
            "would have far better cost-effectiveness.\n",
            "\n",
            "Category 4 talk.politics.mideast: From: jake@bony1.bony.com (Jake Livni)\n",
            "Subject: Re: Why does US consider YIGAL ARENS to be a dangerous to humanity\n",
            "Organization: The Department of Redundancy Department\n",
            "Lines: 31\n",
            "\n",
            "In article <C5sDCK.38n@news.cso.uiuc.edu> eshneken@ux4.cso.uiuc.edu (Edward A Shnekendorf) writes:\n",
            "\n",
            ">Come on!  Most if not all Arabs are sympathetic to the Palestinian war \n",
            ">against Israel.  \n",
            "\n",
            "I wouldn't bet on it.\n",
            "\n",
            "Arab governments generally don't care much about the Palestineans and\n",
            "their struggle but find it useful for political purposes back home.\n",
            "They are happy to leave the Palestineans largely under Israeli control\n",
            "because that leaves the job of controlling them to the Israelis.  The\n",
            "Israelis don't like this job any more than King Hussein of Jordan\n",
            "liked it -- and he managed to kill them off at the rate of thousands\n",
            "per month when they started an Intifada in Jordan.  The governments of\n",
            "Syria, Lebanon and Egypt all feel similarly.  However, proclaiming\n",
            "public support for the Palestinean war against Israel deflects\n",
            "criticism from deep problems at home and lends an air of legitimacy to\n",
            "even the most brutal Arab tyrants.\n",
            "\n",
            "Arab *PEOPLE* probably aren't much more sympathetic.  Palestineans\n",
            "have shown a willingness to destabilize and plunder in Jordan,\n",
            "Lebanon and Kuwait and are viewed with suspicion elsewhere.\n",
            "\n",
            "You might still be right in sympathy to the war against Israel, but I\n",
            "suspect that many Arabs, far removed from the immediate border with\n",
            "Israel (e.g. in Kuwait or Morroco), couldn't care less.\n",
            "\n",
            "-- \n",
            "Jake Livni  jake@bony1.bony.com           Ten years from now, George Bush will\n",
            "American-Occupied New York                   have replaced Jimmy Carter as the\n",
            "My opinions only - employer has no opinions.    standard of a failed President.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --user -U nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdaQ3N1vSiXw",
        "outputId": "aef4c61c-9673-4608-87bf-9af3bb2b6769"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_tokens(tokens):\n",
        "    return [\n",
        "        lemmatizer.lemmatize(w.lower())\n",
        "        for w in tokens\n",
        "        if w.lower() not in stop_words            # remove stopwords\n",
        "        and w not in string.punctuation           # remove punctuation\n",
        "        and w.isalpha()                           # remove numbers / non-alphabetic\n",
        "    ]\n",
        "\n",
        "\n",
        "df['tokenised']=[word_tokenize(x) for x in df['text']]\n",
        "df['filtered'] = df['tokenised'].apply(\n",
        "    lambda tokens: [w for w in tokens if w.lower() not in stop_words]\n",
        ")\n",
        "df['lemmatized']= df['filtered'].apply(lambda texts :clean_tokens(texts))\n",
        "# i used lemmatized instead of stemmed because training is one time cost. And it will increase the quality too\n",
        "\n",
        "df['pos_tags']=df['lemmatized'].apply(lambda token:[pos_tag(token)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rf-icTUlMJaq",
        "outputId": "ab0b9c8c-7212-446e-f82d-769990107217"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "bow_vectorizer = CountVectorizer()\n",
        "X_bow = bow_vectorizer.fit_transform(df['lemmatized'].apply(lambda x: ' '.join(x)))\n",
        "\n",
        "print(\"BoW shape:\", X_bow.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88y9xK7cbEZw",
        "outputId": "f4d97e7d-9af2-4a23-d0a5-6cb2f9676c19"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BoW shape: (2938, 29693)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df['lemmatized'].apply(lambda x: ' '.join(x)))\n",
        "\n",
        "print(\"TF-IDF shape:\", X_tfidf.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPN2jH-vbkZX",
        "outputId": "294aea7b-f6e6-4f74-e600-d9a2337172d6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF shape: (2938, 29693)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "cbow_model = Word2Vec(sentences=df['lemmatized'], vector_size=100, window=5, min_count=2, sg=0)\n",
        "skip_model= Word2Vec(sentences=df['lemmatized'], vector_size=100, window=5, min_count=2, sg=1)\n",
        "print(cbow_model.wv['space'])\n",
        "print(skip_model.wv['space'])\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def doc_vector(tokens, model):\n",
        "    tokens = [t for t in tokens if t in model.wv]  # keep only words in vocab\n",
        "    if len(tokens) == 0:\n",
        "        return np.zeros(model.vector_size)\n",
        "    return np.mean(model.wv[tokens], axis=0)\n",
        "\n",
        "df['doc_vec'] = df['lemmatized'].apply(lambda x: doc_vector(x, cbow_model))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mCJyLXYboMR",
        "outputId": "68f0694b-807f-4b11-fc28-f7e40a4a27e9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.9053326   1.52538     0.62565315 -1.569798    0.4578813  -0.6376697\n",
            "  2.0609922   1.7663788  -0.12770066 -0.5194172   0.09690235 -1.9792314\n",
            "  2.022975    0.9424169   2.238214    0.08094811 -0.07205864  1.1550025\n",
            " -0.6455151  -1.0754907   0.50554883 -1.2737622   0.45528352 -0.60792685\n",
            "  1.5694332  -0.70128703 -0.26925805 -0.19175851 -0.6776452  -2.2995374\n",
            "  0.16506682 -0.03748273 -0.6249189   0.09971326  1.5196351   0.6759365\n",
            " -0.30594096  1.4932548   2.1805696  -1.394475    0.03207586 -0.9855001\n",
            " -0.44926506 -0.32113865 -0.16104682  0.5731132  -0.2521947   0.56789213\n",
            "  1.2281009   0.45331702  0.80161065  0.04377218 -0.44880545 -1.9102753\n",
            "  0.31686133 -0.32898262  0.8351898  -0.4840352  -0.44767937  0.3405744\n",
            "  0.05599922 -0.20480722 -0.02550899  0.9642617  -0.65214795  0.98851526\n",
            " -0.635061   -0.04114639  0.4127789  -0.0261205  -0.81442046 -0.35484567\n",
            "  1.0991627   0.11369587 -0.17492272  2.3146703   0.41382343 -0.80644107\n",
            " -0.08720588  0.65994537 -0.14614217  0.22420658  0.8999574   0.13761869\n",
            "  1.313829    1.0715754   0.5343251   0.15165426  1.7850155   1.305801\n",
            "  1.3059404   0.49623182 -0.01891221 -0.59179014  1.9316516   1.9011348\n",
            " -0.9053194  -1.3804107   1.0814942  -0.35479912]\n",
            "[ 0.36427993  0.74732524  0.56913245 -0.12195163  0.7542753  -0.7371445\n",
            "  1.0580992  -0.10279915 -0.6368224   0.40754038 -0.08618187 -0.6602443\n",
            "  0.30418673 -0.20406497  0.39043128 -0.17775941  0.35081714  0.09966312\n",
            "  0.50355667 -0.95944345  0.13764629 -0.16123593  0.38925216  0.09581877\n",
            "  0.10798498 -0.83505714 -0.8073926   0.3942679  -0.4247389   0.1039859\n",
            " -0.1399091  -0.114676   -0.13470285  0.11246021  0.19544663  0.07079739\n",
            " -0.40785092  0.65337247  0.4893164  -0.00171458  0.4788984  -0.48946065\n",
            "  0.03479064 -0.10050561  0.05892712  0.2109756  -1.2926401  -0.53864616\n",
            "  0.39385453  0.20745823  0.8399584   0.65126103  0.36523345 -0.3405964\n",
            "  0.49544325 -0.2115521   0.19180188  0.091352   -0.27435303 -0.2443543\n",
            "  0.01452115 -0.06819671 -0.43517524  0.39197025  0.18261553  0.388722\n",
            "  0.04006001  0.15371513  0.03693022 -0.04365004  0.1510617  -0.03092793\n",
            "  0.4529141   0.9288434   0.5828758   0.7892199   0.72673535  0.6595399\n",
            " -0.14969887  0.7095089  -0.01575793  0.25131804 -0.12088399 -0.01142244\n",
            "  0.35729218  0.714545    0.44893482  0.10342868  0.8143552   0.67240036\n",
            "  0.48237556  0.33765215  0.74321496 -0.24108611  0.84121644  0.85674506\n",
            " -0.07826362 -0.12536334  0.17051622 -0.5342457 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Convert tokenized/lemmatized lists back to string for vectorizers\n",
        "df['clean_text'] = df['lemmatized'].apply(lambda x: \" \".join(x))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['clean_text'], df['target'], test_size=0.2, random_state=42)\n",
        "\n",
        "# ---- Bag of Words ----\n",
        "bow = CountVectorizer(max_features=5000)\n",
        "X_train_bow = bow.fit_transform(X_train)\n",
        "X_test_bow = bow.transform(X_test)\n",
        "\n",
        "lr_bow = LogisticRegression(max_iter=200)\n",
        "lr_bow.fit(X_train_bow, y_train)\n",
        "print(\"BoW + Logistic Regression\")\n",
        "print(classification_report(y_test, lr_bow.predict(X_test_bow)))\n",
        "\n",
        "# ---- TF-IDF ----\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "lr_tfidf = LogisticRegression(max_iter=200)\n",
        "lr_tfidf.fit(X_train_tfidf, y_train)\n",
        "print(\"TF-IDF + Logistic Regression\")\n",
        "print(classification_report(y_test, lr_tfidf.predict(X_test_tfidf)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbd8-VZSb9Rd",
        "outputId": "fb7feca2-5b18-4188-f723-661f320eaabd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BoW + Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.97      0.95       119\n",
            "           1       0.96      0.95      0.95       117\n",
            "           2       0.98      0.96      0.97       106\n",
            "           3       0.97      0.98      0.97       128\n",
            "           4       0.99      0.95      0.97       118\n",
            "\n",
            "    accuracy                           0.96       588\n",
            "   macro avg       0.96      0.96      0.96       588\n",
            "weighted avg       0.96      0.96      0.96       588\n",
            "\n",
            "TF-IDF + Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.99      0.97       119\n",
            "           1       0.98      0.97      0.98       117\n",
            "           2       0.99      0.98      0.99       106\n",
            "           3       0.98      0.98      0.98       128\n",
            "           4       1.00      0.97      0.99       118\n",
            "\n",
            "    accuracy                           0.98       588\n",
            "   macro avg       0.98      0.98      0.98       588\n",
            "weighted avg       0.98      0.98      0.98       588\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "# Tokenizer for sequences\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(df['clean_text'])\n",
        "\n",
        "X_seq = tokenizer.texts_to_sequences(df['clean_text'])\n",
        "X_pad = pad_sequences(X_seq, maxlen=200)  # pad to equal length\n",
        "y = df['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ---- Simple LSTM ----\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=10000, output_dim=128, input_length=200))\n",
        "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(len(df['target_name']), activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))\n",
        "\n",
        "print(\"LSTM Test Accuracy:\", model.evaluate(X_test, y_test)[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKdR3ZaKdWn6",
        "outputId": "0c44f63a-60f6-42e1-de84-d6dac6ea0957"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 368ms/step - accuracy: 0.2189 - loss: 7.6800 - val_accuracy: 0.1803 - val_loss: 4.2852\n",
            "Epoch 2/5\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 355ms/step - accuracy: 0.2226 - loss: 3.0475 - val_accuracy: 0.1990 - val_loss: 1.6867\n",
            "Epoch 3/5\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 356ms/step - accuracy: 0.1837 - loss: 1.6673 - val_accuracy: 0.1803 - val_loss: 1.6435\n",
            "Epoch 4/5\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 351ms/step - accuracy: 0.1963 - loss: 1.6396 - val_accuracy: 0.1803 - val_loss: 1.6354\n",
            "Epoch 5/5\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 355ms/step - accuracy: 0.2025 - loss: 1.6300 - val_accuracy: 0.1803 - val_loss: 1.6325\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.1563 - loss: 1.6360\n",
            "LSTM Test Accuracy: 0.18027210235595703\n"
          ]
        }
      ]
    }
  ]
}