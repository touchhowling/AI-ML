{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f07dc34",
   "metadata": {},
   "source": [
    "You will learn:\n",
    "\n",
    "* Preprocess text using Python\n",
    "* Convert text into numeric format using BoW, TF-IDF, and embeddings\n",
    "* Understand how CBoW and Skip-gram work through coding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0c4fb4",
   "metadata": {},
   "source": [
    "## Part 1: Text Preprocessing with nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40986025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Deep', 'learning', 'models', 'are', 'powerful', 'tools', 'for', 'natural', 'language', 'processing', '.']\n",
      "----------------------------------------------------------------------------------------------------------------------\n",
      "Without Stopwords: ['Deep', 'learning', 'models', 'powerful', 'tools', 'natural', 'language', 'processing', '.']\n",
      "----------------------------------------------------------------------------------------------------------------------\n",
      "Stemmed: ['deep', 'learn', 'model', 'power', 'tool', 'natur', 'languag', 'process', '.']\n",
      "----------------------------------------------------------------------------------------------------------------------\n",
      "Lemmatized: ['Deep', 'learning', 'model', 'powerful', 'tool', 'natural', 'language', 'processing', '.']\n",
      "----------------------------------------------------------------------------------------------------------------------\n",
      "POS Tags: [('Deep', 'NNP'), ('learning', 'NN'), ('models', 'NNS'), ('are', 'VBP'), ('powerful', 'JJ'), ('tools', 'NNS'), ('for', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "text = \"Deep learning models are powerful tools for natural language processing.\"\n",
    "tokens = word_tokenize(text)\n",
    "filtered = [w for w in tokens if w.lower() not in stopwords.words('english')]\n",
    "stemmed = [PorterStemmer().stem(w) for w in filtered]\n",
    "lemmatized = [WordNetLemmatizer().lemmatize(w) for w in filtered]\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"----------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"Without Stopwords:\", filtered)\n",
    "print(\"----------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"Stemmed:\", stemmed)\n",
    "print(\"----------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"Lemmatized:\", lemmatized)\n",
    "print(\"----------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"POS Tags:\", pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f991edc",
   "metadata": {},
   "source": [
    "## Part 2: Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38a4dbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW:\n",
      " [[1 0 0 1 0 1 0 0 0 1 0]\n",
      " [0 1 0 1 1 0 0 1 0 0 1]\n",
      " [1 0 1 0 0 1 1 0 1 0 0]]\n",
      "TF-IDF:\n",
      " [[0.45985353 0.         0.         0.45985353 0.         0.45985353\n",
      "  0.         0.         0.         0.60465213 0.        ]\n",
      " [0.         0.46735098 0.         0.35543247 0.46735098 0.\n",
      "  0.         0.46735098 0.         0.         0.46735098]\n",
      " [0.37302199 0.         0.49047908 0.         0.         0.37302199\n",
      "  0.49047908 0.         0.49047908 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"Deep learning is powerful.\",\n",
    "    \"Natural language processing is fun!\",\n",
    "    \"Deep learning models help NLP.\"\n",
    "]\n",
    "\n",
    "# Bag of Words\n",
    "bow = CountVectorizer()\n",
    "print(\"BoW:\\n\", bow.fit_transform(corpus).toarray())\n",
    "\n",
    "# TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "print(\"TF-IDF:\\n\", tfidf.fit_transform(corpus).toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8cb2ac",
   "metadata": {},
   "source": [
    "## Part 3: Word Embeddings + Intro to CBoW & Skip-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8a643d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBoW - Vector for 'deep': [ 1.56351421e-02 -1.90203730e-02 -4.11062239e-04  6.93839323e-03\n",
      " -1.87794445e-03  1.67635437e-02  1.80215668e-02  1.30730132e-02\n",
      " -1.42324204e-03  1.54208085e-02 -1.70686692e-02  6.41421322e-03\n",
      " -9.27599426e-03 -1.01779103e-02  7.17923651e-03  1.07406788e-02\n",
      "  1.55390287e-02 -1.15330126e-02  1.48667218e-02  1.32509926e-02\n",
      " -7.41960062e-03 -1.74912829e-02  1.08749345e-02  1.30195115e-02\n",
      " -1.57510047e-03 -1.34197120e-02 -1.41718509e-02 -4.99412045e-03\n",
      "  1.02865072e-02 -7.33047491e-03 -1.87401194e-02  7.65347946e-03\n",
      "  9.76895820e-03 -1.28571270e-02  2.41711619e-03 -4.14975407e-03\n",
      "  4.88066689e-05 -1.97670180e-02  5.38400887e-03 -9.50021297e-03\n",
      "  2.17529293e-03 -3.15244915e-03  4.39334614e-03 -1.57631524e-02\n",
      " -5.43436781e-03  5.32639725e-03  1.06933638e-02 -4.78302967e-03\n",
      " -1.90201886e-02  9.01175756e-03]\n",
      "Skip-Gram - Vector for 'deep': [ 1.56351421e-02 -1.90203730e-02 -4.11062239e-04  6.93839323e-03\n",
      " -1.87794445e-03  1.67635437e-02  1.80215668e-02  1.30730132e-02\n",
      " -1.42324204e-03  1.54208085e-02 -1.70686692e-02  6.41421322e-03\n",
      " -9.27599426e-03 -1.01779103e-02  7.17923651e-03  1.07406788e-02\n",
      "  1.55390287e-02 -1.15330126e-02  1.48667218e-02  1.32509926e-02\n",
      " -7.41960062e-03 -1.74912829e-02  1.08749345e-02  1.30195115e-02\n",
      " -1.57510047e-03 -1.34197120e-02 -1.41718509e-02 -4.99412045e-03\n",
      "  1.02865072e-02 -7.33047491e-03 -1.87401194e-02  7.65347946e-03\n",
      "  9.76895820e-03 -1.28571270e-02  2.41711619e-03 -4.14975407e-03\n",
      "  4.88066689e-05 -1.97670180e-02  5.38400887e-03 -9.50021297e-03\n",
      "  2.17529293e-03 -3.15244915e-03  4.39334614e-03 -1.57631524e-02\n",
      " -5.43436781e-03  5.32639725e-03  1.06933638e-02 -4.78302967e-03\n",
      " -1.90201886e-02  9.01175756e-03]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentences = [word_tokenize(doc.lower()) for doc in corpus]\n",
    "\n",
    "# CBoW: sg=0\n",
    "cbow_model = Word2Vec(sentences, vector_size=50, window=2, min_count=1, sg=0)\n",
    "print(\"CBoW - Vector for 'deep':\", cbow_model.wv['deep'])\n",
    "\n",
    "# Skip-Gram: sg=1\n",
    "skip_model = Word2Vec(sentences, vector_size=50, window=2, min_count=1, sg=1)\n",
    "print(\"Skip-Gram - Vector for 'deep':\", skip_model.wv['deep'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc217b6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IMDB dataset...\n",
      "Training data: 25000 reviews\n",
      "Testing data: 25000 reviews\n",
      "\n",
      "Sample review excerpt:\n",
      "this film was just brilliant casting location scenery story direction everyone's really suited the p...\n",
      "Sentiment: Positive\n",
      "\n",
      "Demonstrating text preprocessing:\n",
      "Tokens: ['this', 'film', 'was', 'just', 'brilliant', 'casting', 'location', 'scenery', 'story', 'direction']\n",
      "Filtered: ['film', 'brilliant', 'casting', 'location', 'scenery', 'story', 'direction', 'everyone', \"'s\", 'really']\n",
      "Stemmed: ['film', 'brilliant', 'cast', 'locat', 'sceneri', 'stori', 'direct', 'everyon', \"'s\", 'realli']\n",
      "Lemmatized: ['film', 'brilliant', 'casting', 'location', 'scenery', 'story', 'direction', 'everyone', \"'s\", 'really']\n",
      "\n",
      "Demonstrating text vectorization:\n",
      "BoW shape: (3, 260)\n",
      "First 5 features: ['1990s' '80' 'abomination' 'across' 'acting']\n",
      "TF-IDF shape: (3, 260)\n",
      "\n",
      "Demonstrating word embeddings:\n",
      "Embeddings for word 'the':\n",
      "CBoW (first 5 dims): [-0.00099518  0.00052011  0.01024346  0.01807732 -0.01851844]\n",
      "Skip-gram (first 5 dims): [-0.00105838  0.00053453  0.01102884  0.01777286 -0.01856817]\n",
      "\n",
      "Preparing data for LSTM model:\n",
      "Training data shape after padding: (25000, 200)\n",
      "Creating standard LSTM model\n",
      "Model has 1411713 total parameters\n",
      "\n",
      "Training model...\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kahilamokhtarijadid/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 312ms/step - accuracy: 0.6651 - loss: 0.6018 - val_accuracy: 0.7998 - val_loss: 0.4427\n",
      "Epoch 2/3\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 319ms/step - accuracy: 0.8513 - loss: 0.3592 - val_accuracy: 0.8452 - val_loss: 0.3678\n",
      "Epoch 3/3\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 303ms/step - accuracy: 0.8739 - loss: 0.3127 - val_accuracy: 0.8284 - val_loss: 0.3854\n",
      "\n",
      "Evaluating model...\n",
      "Test accuracy: 0.8474\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 39ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.88      0.81      0.84     12500\n",
      "    Positive       0.82      0.88      0.85     12500\n",
      "\n",
      "    accuracy                           0.85     25000\n",
      "   macro avg       0.85      0.85      0.85     25000\n",
      "weighted avg       0.85      0.85      0.85     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# NLP tools\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ===============================================================\n",
    "# PART 1: Load Dataset\n",
    "# ===============================================================\n",
    "print(\"Loading IMDB dataset...\")\n",
    "\n",
    "max_features = 10000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(f\"Training data: {len(x_train)} reviews\")\n",
    "print(f\"Testing data: {len(x_test)} reviews\")\n",
    "\n",
    "# Get word dictionary to decode reviews\n",
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = {value: key for key, value in word_index.items()}\n",
    "\n",
    "# Function to decode review\n",
    "def decode_review(encoded_review):\n",
    "    # Skip special tokens and unknown words instead of using '?'\n",
    "    words = [reverse_word_index.get(i-3, '') for i in encoded_review]\n",
    "    # Filter out empty strings\n",
    "    return ' '.join([word for word in words if word])\n",
    "\n",
    "print(\"\\nSample review excerpt:\")\n",
    "sample_review = decode_review(x_train[0])\n",
    "print(sample_review[:100] + \"...\")\n",
    "print(f\"Sentiment: {'Positive' if y_train[0] == 1 else 'Negative'}\")\n",
    "\n",
    "# ===============================================================\n",
    "# PART 2: Text Preprocessing Demo\n",
    "# ===============================================================\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Demonstrate NLP preprocessing steps\"\"\"\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered = [w for w in tokens if w not in stop_words]\n",
    "    \n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed = [stemmer.stem(w) for w in filtered]\n",
    "    \n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(w) for w in filtered]\n",
    "    \n",
    "    return {\n",
    "        'tokens': tokens[:10], \n",
    "        'filtered': filtered[:10], \n",
    "        'stemmed': stemmed[:10], \n",
    "        'lemmatized': lemmatized[:10]\n",
    "    }\n",
    "\n",
    "# Process a sample review\n",
    "print(\"\\nDemonstrating text preprocessing:\")\n",
    "processed = preprocess_text(sample_review)\n",
    "for key, value in processed.items():\n",
    "    print(f\"{key.capitalize()}: {value}\")\n",
    "\n",
    "# ===============================================================\n",
    "# PART 3: Text Vectorization Demo\n",
    "# ===============================================================\n",
    "print(\"\\nDemonstrating text vectorization:\")\n",
    "\n",
    "# Create a small corpus for demonstration\n",
    "small_corpus = [decode_review(x) for x in x_train[:3]]\n",
    "\n",
    "# Bag of Words\n",
    "bow = CountVectorizer(max_features=1000)\n",
    "bow_matrix = bow.fit_transform(small_corpus)\n",
    "print(f\"BoW shape: {bow_matrix.shape}\")\n",
    "print(f\"First 5 features: {bow.get_feature_names_out()[:5]}\")\n",
    "\n",
    "# TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=1000)\n",
    "tfidf_matrix = tfidf.fit_transform(small_corpus)\n",
    "print(f\"TF-IDF shape: {tfidf_matrix.shape}\")\n",
    "\n",
    "# ===============================================================\n",
    "# PART 4: Word Embeddings Demo\n",
    "# ===============================================================\n",
    "print(\"\\nDemonstrating word embeddings:\")\n",
    "\n",
    "# Tokenize corpus for Word2Vec\n",
    "tokenized_corpus = [word_tokenize(text.lower()) for text in small_corpus]\n",
    "\n",
    "# Train Word2Vec models\n",
    "cbow_model = Word2Vec(tokenized_corpus, vector_size=50, window=2, min_count=1, sg=0)\n",
    "skipgram_model = Word2Vec(tokenized_corpus, vector_size=50, window=2, min_count=1, sg=1)\n",
    "\n",
    "# Show embeddings for a sample word\n",
    "vocab = list(cbow_model.wv.index_to_key)\n",
    "if vocab:\n",
    "    sample_word = vocab[0]\n",
    "    print(f\"Embeddings for word '{sample_word}':\")\n",
    "    print(f\"CBoW (first 5 dims): {cbow_model.wv[sample_word][:5]}\")\n",
    "    print(f\"Skip-gram (first 5 dims): {skipgram_model.wv[sample_word][:5]}\")\n",
    "\n",
    "# ===============================================================\n",
    "# PART 5: LSTM Model for Sentiment Analysis\n",
    "# ===============================================================\n",
    "print(\"\\nPreparing data for LSTM model:\")\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "maxlen = 200\n",
    "x_train_pad = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test_pad = pad_sequences(x_test, maxlen=maxlen)\n",
    "print(f\"Training data shape after padding: {x_train_pad.shape}\")\n",
    "\n",
    "# Create LSTM model\n",
    "print(\"Creating standard LSTM model\")\n",
    "model = Sequential([\n",
    "    # Embedding layer converts integer indices to dense vectors\n",
    "    # Add input_shape to ensure model is built immediately\n",
    "    Embedding(max_features, 128, input_shape=(maxlen,)),\n",
    "    \n",
    "    # LSTM layer processes the sequence\n",
    "    LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
    "    \n",
    "    # Output layer with sigmoid activation for binary classification\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Print summary with parameter counts\n",
    "print(f\"Model has {model.count_params()} total parameters\")\n",
    "# Skip full summary display\n",
    "# model.summary()\n",
    "\n",
    "# Train model\n",
    "print(\"\\nTraining model...\")\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    x_train_pad, y_train,\n",
    "    batch_size=128,\n",
    "    epochs=3,  # Small number for demonstration\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "print(\"\\nEvaluating model...\")\n",
    "loss, accuracy = model.evaluate(x_test_pad, y_test, verbose=0)\n",
    "print(f\"Test accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Generate predictions\n",
    "y_pred = (model.predict(x_test_pad) > 0.5).astype(int).flatten()\n",
    "\n",
    "# Show classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a083a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
